---
title: "Why Coordination Fails Among People Who Agree"
date: 2026-02-20
summary: "Coordination fails among people who agree on values because they're running different models of how the world works."
---

## tl;dr

Coordination fails among people who agree on values because they're running different models of how the world works. New information doesn't update those models - it gets interpreted through them. This is why better messaging, clearer data, and more trusted messengers mostly don't work.

- The real diagnostic: when people who want the same things can't coordinate, the models are incompatible, not the values. AI policy is a vivid current example, but the pattern is general.
- Interventions aimed at behavior miss the model entirely. Design from the model out, not the behavior in.
- **Model-First Policy** maps how each stakeholder group sees the world before designing interventions, identifies actual value overlap, and traces what each proposal signals within each worldview.

---

## The Agreement Problem

There's a particular kind of frustration that comes from watching people who want the same thing fail to act together. It's different from the ordinary frustration of political opposition, where at least the disagreement is honest. This is something more disorienting: shared values, shared stakes, shared urgency, and still no coordinated action. Just people talking past each other while the window closes.

## When Models Diverge

AI policy is full of this right now. The debates that dominate public discourse, in federal hearings and state legislatures and the comment sections of technical blogs, look like values fights. Techno-optimists against precautionary skeptics, growth against governance, acceleration against restraint. But spend time with the people actually working on these questions and a different picture emerges. Most of them want roughly the same things: benefits that don't flow exclusively to the people who least need them, accountability that has some teeth, decisions made by people who bear some portion of the consequences. The values overlap is real and surprisingly broad.

What they don't share is a model of how the world works.

Daily AI users and people who've never touched these tools have been shaped by entirely different experiences into entirely different assumptions. For someone who uses Claude or GPT daily, the technology is concrete, its limitations are visible, and its potential to extend human capability is not abstract. For someone who hasn't, it's a thing that happens to other people, narrated by institutions with track records that invite skepticism. The same policy proposal lands differently depending on the model behind it. A datacenter moratorium looks like sensible precaution to someone whose model says extraction is the default pattern of tech development, and like obstruction dressed as environmentalism to someone whose model says infrastructure scarcity is the binding constraint on beneficial applications. Same values, different models, zero coordination.

## Why Better Messaging Doesn't Work

The standard response to this is better communication. Clearer messaging. More accessible explanations of the technology, better data on impacts, more trusted messengers. This is well-intentioned and it mostly doesn't work, for a reason that's been well understood in cognitive science for decades but hasn't made it into policy practice.

People don't update their behavior when you change the incentive structure, or clarify the information, or find a more trusted messenger. They update their *model*, and then behavior follows. When new information arrives, nobody evaluates it against some neutral baseline and revises accordingly. They evaluate it against the model they already hold and, in most cases, find a way to interpret the new information that leaves the model intact. The subsidy signals scarcity to the person whose model says government intervention marks failing systems. The awareness campaign signals condescension to the person whose model says they already understand the situation better than the campaign's designers. The intervention lands, but nothing changes, because the model wasn't reached.

This is the mechanism behind a failure mode that every experienced policy practitioner has felt but rarely has language for. You design an intervention carefully, you pilot it, it appears to work, and then the effect evaporates or reverses or simply fails to scale. The post-mortem arrives at "messaging problem" or "implementation gap" because those are the categories the standard toolkit offers. But the actual problem, most of the time, is that you aimed at behavior and missed the model entirely.

## The Base Rate Problem

The communities bearing the most direct costs of AI infrastructure expansion understand this intuitively, even if they don't use this vocabulary. Their experience of technology development is a long series of promises whose benefits went elsewhere while the costs stayed local. The model they've built from that evidence predicts how these situations resolve. A new proposal, however well-designed, interacts with that model. The prediction it generates will align with their existing models. This is a protective move, because the cost of holding it is zero, while the cost of updating it requires a genuine underlying belief change, which is always psychologically costly.

No amount of better messaging reaches that existing model. What reaches it is an intervention designed from the inside out, starting with the model people are actually running rather than the behavior the designer wants to produce.

## Mapping the Coordination Surface

People on opposite sides of AI policy debates share more values than they realize. When people who share values still can't coordinate, that's a diagnostic signal: the models are incompatible, and each model generates different predictions about what any given intervention will actually do. Knowing where those predictions diverge tells you where the intervention needs to land if it's going to work at all. Making that divergence visible is the precondition for coordination, and it's the step that almost never gets taken.

This is what Model-First Policy does. Before designing an intervention, map how each relevant group actually sees the situation. Identify where the values overlap, as opposed to where the rhetoric claims they do. Trace what each intervention signals within each worldview, what response it's likely to generate, and where the models have enough in common to act on. The output is a map of the coordination surface - the territory where action is actually possible given how people actually think.

The pattern applies well beyond AI policy. Any coordination problem among stakeholders who nominally agree but can't act together is a candidate for this diagnostic. The models are always there, always doing the interpretive work, and almost never made explicit. Making them explicit is the first move. Designing interventions that work *within* those models, rather than trying to replace them, is the second. The coordination follows.
